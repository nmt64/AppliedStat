---
title: "Annotated Appendix _ EDA Final Project Report"
author: "Tam Nguyen"
date: "October 28th, 2018"
output:
  word_document: default
  html_document:
    df_print: paged
always_allow_html: yes
---
```{r include=FALSE}
library(mosaic); library(readr);
library(doMC)
library(dplyr)
library(purrr)
library(caret)
library(corrplot)
library(knitr)
library(plotly)
train = read_csv("/Users/Tam/Documents/Junior1819/Fall2018/Stat/AppliedStat/FinalStat/TamNguyen DataSet_Stage1/train.csv");
```



```{r include = FALSE}
str(train)
```


### Exploring missing value

```{r}
nrows <- nrow(train) #Numer of rows in train dataset
missing <- sort(map_dbl(train, function(x) sum(is.na(x)) / nrows), decreasing = TRUE)
names_missing <- names(missing[missing > 0])
head(missing, 20)
```
```{r}
summary(missing)
plot(missing)
```

- Nineteen (19) of the original 80 variables have some degree of missing values. To begin addressing this problem we work on x_data for this analysis.

```{r}
#Make an copy of train dataset to work on 19 variables seprately 
x_data <- train
names_missing_del <- names(missing[missing > 0.8])
x_data <- select(x_data, one_of(setdiff(names(x_data),names_missing_del ))) #new training dataset without 4 highest missing predictors
```


```{r}
sum(is.na(train)) #how many data is missing
sum(is.na(train))/(1460*80) #missing value percentage
```

### Continuous predictor

For the initial iteration of the problem we first focus on those continuous predictor values. An investigation will be carried out to find good performing models with a focus on identifying (if any) the gap between simple explainable models and the more complex predictive models.

```{r}
num_data <- select_if(x_data, is.numeric); 
summary(num_data)
nrow(num_data); ncol(num_data)
```

### Low variance variables

Max Kuhn (2016):
Given this, a rule of thumb for detecting near-zero variance predictors is:
- The fraction of unique values over the sample size is low (say 10 %).
- The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (say around 20).

```{r}
#Function calculating the fraction of unique values over the sample size and the ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value
condition <- function(x) {
  checking = list()  #emty list
  tbl = sort(table(x), decreasing = TRUE) #Sorting table decreasing
  checking[["unique_to_samp"]] = length(tbl) / sum(tbl) # Get the variance by                                         divide the length of table to sum of the table
  checking[["most_prev_to_2nd_prev"]] = (tbl[[1]] / tbl[[2]]) #get ratio
  checking
}

#Function checking if unique_to_samp < 0.1 and                                  most_prev_to_2nd_prev >= 20  
low_var <- function(x) {
  low_var_vec = vector("character", ncol(x))
  i = 1
  for (nme in names(x)) {
    obs = condition(x[[nme]])
    #print(obs)  #test by printing value
    if (obs[[1]] <= 0.1 & obs[[2]] >= 20) { 
      low_var_vec[i] = nme
      i = i + 1
    }
  }
  low_var_vec[low_var_vec != ""]
}

degen_vec <- low_var(num_data); degen_vec
num_data <- select(num_data, one_of(setdiff(names(num_data), degen_vec))) #make a new dataset without low variance variables
```
```{r}
pairs(~SalePrice + BsmtFinSF2+LowQualFinSF + KitchenAbvGr + EnclosedPorch+ ScreenPorch+PoolArea+MiscVal,data=train)
```


### Multicolinear

The idea is to first remove the predictors that have the most correlated relationships.
- Calculate the correlation matrix of the predictors
- Determine the two predictors associated with the largest absolute pairwise correlation (call them predictors A and B).
- Determine the average correlation between A and the other variables. Do the same for predictor B.
- If A has a larger average correlation, remove it; otherwise, remove predictor B.
- Repeat Steps 2–4 until no absolute correlations are above the threshold.

```{r}
get_collinear <- function(x) {
  # Expects data dataframe
  num_cols = ncol(x)
  collinear_vec = vector("character", num_cols) 
  index = 1
  
  for (i in seq(1:num_cols)) {
    corMat = cor(x)
    diag(corMat) = 0  #set diagonal = 0
    df_cols = names(x)
    #Determine the two predictors associated with the largest absolute pairwise     correlation (call them predictors A and B).
    AB = which(corMat == max(abs(corMat), na.rm=TRUE), arr.ind = TRUE)
    if (corMat[AB][[1]] > 0.75) {
      names_AB = rownames(AB)
      
      if (sum(abs(corMat[names_AB[1], ]),na.rm=TRUE) > sum(abs(corMat[names_AB[2],  ]),na.rm=TRUE)) {
        
        collinear_vec[index] = names_AB[1]
        index = index + 1
      } 
      # if pairwise correlations less than 0.75
      else {collinear_vec[index] = names_AB[2]
            index = index + 1}
      
      x = select(x, one_of(setdiff(df_cols, collinear_vec[index - 1])))
    }
    else{break} 
  }
  collinear_vec[collinear_vec != ""]
}
mul_col = get_collinear(num_data); mul_col
```
```{r}
plot(train$GrLivArea, train$SalePrice)
```

```{r}
#Correlation matrix for 26 continuous variables
num_data <- select(num_data, one_of(setdiff(names(num_data), mul_col)))
num_data$GrLivArea = train$GrLivArea #Adding GrLivArea back
corrplot(cor(num_data, use = "pairwise.complete.obs"), method = "ellipse", tl.col = "black", na.label = T)
```

### Decode variables: 

```{r}
copy_cont_var = num_data  #copy the continuous dataset to add more variables

#Checking if having lotshape condition is useable or not
copy_cont_var$LotShape_new <- ifelse(train$LotShape == 'IR3',0,1)

#Checking if having basement exposure or not
copy_cont_var$BsmtExposure_new <- ifelse(train$BsmtExposure == 'No',0,1)
copy_cont_var$BsmtExposure_new[is.na(copy_cont_var$BsmtExposure_new)] = 0 #Change NA value = 0

```

```{r}
#Checking if having full bath or halfbath
copy_cont_var$FullBath <- ifelse(train$BsmtFullBath > 0,1,0)
copy_cont_var$HalfBath <- ifelse(train$BsmtHalfBath > 0,1,0)

#Checking if having other Miscellaneous or not
copy_cont_var$MiscFeature_new = ifelse(train$MiscFeature == 'NA',0,1)
copy_cont_var$MiscFeature_new[is.na(copy_cont_var$MiscFeature_new)] = 0 #Change NA value = 0

#Checking if having fireplace or not
copy_cont_var$Fireplace = ifelse(train$Fireplaces > 0,1,0)

#Checking if having garage or not
copy_cont_var$GarageYrBlt = ifelse(copy_cont_var$GarageYrBlt == 'NA',0,1)
copy_cont_var$GarageYrBlt[is.na(copy_cont_var$GarageYrBlt)] = 0 #Change NA value = 0

#Checking if having porch/wood desk ...
copy_cont_var$WoodDeckSF <- as.numeric(copy_cont_var$WoodDeckSF)
copy_cont_var$OpenPorchSF <- as.numeric(copy_cont_var$OpenPorchSF)
copy_cont_var$Porch = copy_cont_var$WoodDeckSF + copy_cont_var$OpenPorchSF
copy_cont_var$Porch = ifelse(copy_cont_var$Porch > 0, 1, 0)  #Change to binary var

#Deleting var
copy_cont_var$WoodDeckSF = NULL
copy_cont_var$OpenPorchSF = NULL
copy_cont_var$Id = NULL
copy_cont_var$LotFrontage = NULL
copy_cont_var$YearBuilt = NULL
copy_cont_var$MoSold = NULL
```

```{r}
#Relationship between OverallCond vs SalePrice and Overall Quality Condition vs Sale Price
qual.df <- x_data[ ,c("OverallQual","OverallCond","SalePrice")]
pl.q <- plot_ly(qual.df, y = ~SalePrice, x = ~OverallQual, 
                type = "box", name = "Overall Quality")
pl.c <- plot_ly(qual.df, y = ~SalePrice, x = ~OverallCond, 
                type = "box", name = "Overall Condition")
subplot(pl.q, pl.c)
```


### Citation 
De Cock, Dean. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19, no. 3 (November 2011). https://doi.org/10.1080/10691898.2011.11889627.
 
Kuhn, Max, and Kjell Johnson. “Data Pre-Processing.” In Applied Predictive Modeling, edited by Max Kuhn and Kjell Johnson, 27–59. New York, NY: Springer New York, 2013. https://doi.org/10.1007/978-1-4614-6849-3_3.

“Information About Factors That Determine Property Prices - HomeGuru.” Accessed October 18, 2018. http://www.homeguru.com.au/house-prices/.
